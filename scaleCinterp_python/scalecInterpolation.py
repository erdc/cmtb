# Autogenerated with SMOP version 0.25... kinda... not really
# main.py scalecInterpPerturbations.m -o scalecInterpPerturbations.py
import numpy as np
from .supportingMethods import consistentWeight
from scipy import interpolate

def scalecInterpPerturbations(x, z, s, xi, lx, filtername, nmseitol, Ntotal, Ndone):
    """DOES NOT REMOVE A POLYNOMIAL TREND, AS TREND IS ASSUMED ALREADY REMOVED FROM
     PERTURBATION DATA, DEFAULTS TO ZERO VALUE IF NO DATA
    
     Args:
         x: the nxm location of the data - repeated indices not handled well (ignored)
         z: the observations MINUS a TREND surface
         s: the the observation errors (i.e., standard deviations, rms, etc.)
             s is used to weight the observations as 1/s
             choose s=0 if no information is available about the weights

         xi: the interpolation locations
         lx: the interpolation weighting length scales
         filtername: the name of a filter to analyze:
             'quadloess'
             'linloess'
             'hanning'
             'boxcar'

         nmseitol: a maximum error level, if exceeded causes doubling of smoothing scales
               NOTE: if nmseitol=1 then this means we accept result with input scales
                     if nmseitol<1 then this means interpolation will successive doubling of scales to reach desired noise reduction

         Ntotal: the total number of interpolated points being processed (e.g., larger than N if doing tiles)
         Ndone: the total number of interpolated points already processed (e.g., number done in previous tiles)

     Returns
       zi: the estimate
       msei: the mean square interpolation error estimate (units of z)
       nmsei: the normalized mean square error
       msri: the mean square residuals

    """
    # deal with input
    N, m = np.shape(x)
    Ni, mi = np.shape(xi)

    if (Ntotal == None):  # (nargin_() < 8 | (not Ntotal)):
        Ntotal = Ni
    if (Ndone == None):  # (nargin_() < 9 | (not Ndone)):
        Ndone = 0

    # Fix up s, if constant
    if (np.size(s) == 1):
        s = np.tile(s, (N, 1))
    
    assert type(x) != np.ma.array, 'scaleCinterpolation cannot handle masked items, logic needs to change'
    assert type(z) != np.ma.array, 'scaleCinterpolation cannot handle masked items, logic needs to change'
    assert type(s) != np.ma.array, 'scaleCinterpolation cannot handle masked items, logic needs to change'

    # Deal with nans, this logic would need to change from looking from nans to looking for masked values
    tmp = np.concatenate((x, z, s), axis=1)
    idd = (np.ravel(np.isfinite(np.sum(tmp, axis=1))).nonzero())[0]
    del tmp
    x = x[idd, :]
    z = z[idd]
    s = s[idd]
    if (np.shape(lx) == N and np.shape(lx) != Ni):
        # got to remove the corresponding scales
        lx = lx[idd, :]
    ## now done with that, 'we have clean data ie no nans no masked values'
    N, m = np.shape(x)

    # Use weighted calculations on DATA
    wtol = 0.1  # tolerance for iterative convergence of weights
    s = s ** 2  # need variance, not standard deviation

    wt, var_z = consistentWeight(z, s, wtol)
    # normalize weights
    wt = (wt + np.spacing(1)) / (np.spacing(1) + max(wt))

    # eliminate useless variables -sb
    # tmp = np.array([])
    # for i in xrange(0,m):
        # tmp = np.append(tmp, np.std(x[:,i]))
    std_x = np.std(x, axis=0) #tmp
    # del tmp
    idd = np.argwhere(std_x == 0).squeeze()
    if idd.size > 0: # catch variables with zero variance (e.g., a profile)
        std_x[idd] = 1 + 0*idd
    
    # get the convolution kernel at adequate resolution
    Rmax = 1
    Dr = 0.01 # Need to descretize the continuous kernel function
    if (nmseitol == None): 
        nmseitol = np.inf # Never invoke tolerance 
        print('scalecInterpPerturbations: Setting maximum nmse tolerance to ', nmseitol)
    ###### Set appropriate switches for interpolation based on input
    if 'quadloess' == filtername:
        # MUST DO THIS CORRECTLY FOR N-D
        from .supportingMethods import loess_kernelND
        ri, ai = loess_kernelND(m, 2)
        Fc = 0.7 # Here is the half-power point
    elif 'linloess' == filtername:
        from .supportingMethods import loess_kernelND
        ri, ai = loess_kernelND(m, 1)
        Fc = 0.4 # Here is the half-power point
    elif 'hanning' == filtername:
        from .supportingMethods import hanning_wt
        ri = np.arange(0, (Rmax+Dr*2), Dr)  # Dr*2 fixes missing end point when compared with matlab
        ai = hanning_wt(ri)
        Fc = 0.4
    elif 'boxcar' == filtername:
        ri = np.arange(0, (Rmax+Dr), Dr)
        ai = np.ones((len(ri), 1), float)
        Fc = 0.25
    elif 'si' == filtername:
        ri = np.arange(0, (Rmax+Dr), Dr)
        ai = np.ones((len(ri), 1), float)
        Fc = 1
    else:
        print('Filter window, ', filtername, ' not found')
        # Change the shape of ri and ai for compatibility
    #    ri = np.reshape(ri, (len(ri),)) # go from shape (len(ri),1) to (len(ri),)... if of that shape
    #    ai = np.reshape(ai, (len(ai),)) # go from shape (len(ai),1) to (len(ai),)... if of that shape
    # ri = ri[:,np.newaxis]
    # initialize output
    zi = np.zeros((Ni,1), float)
    nmsei = np.ones((Ni,1), float)
    msei = np.ones((Ni,1), float)
    msri = np.ones((Ni,1), float)

    # Scale the data for interpolation
    for i in xrange(0, Ni):
        # center on point of interest
        y = x - (np.ones((N, 1), float) * xi[i, :])
        # raise NotImplementedError('Check your smoothing uni/bi directional')
        # now scale if using individual smoothing scales
        if (len(lx) == m):
            # constant scale obs. and interp. locations
            y = np.dot(y, np.diag(1.0 / lx))
        elif (len(lx) == N):
            # scale the data
            y = y / lx
        elif (len(lx) == Ni):
            y = np.dot(y, np.diag(1.0 / lx[i, :]))
        else:
            print('smoothing scales not interpreted: N=%d, Ni=%d, Lx=%dx%d' %(N, Ni, lx.shape[0], lx.shape[1]))

        # Convert input to radial
        r = np.sqrt(np.sum(y ** 2, axis=1))

        # expand smoothing scales until tolerance met
        cnt = 0
        while ((nmsei[i] > nmseitol and cnt < 10) or cnt == 0):  # do it once, at least
            p = 2 ** cnt

            # allow for computationally intensive methods here
            cnt += 1
            if 'si' == filtername:
                from supportingMethods import si_wt
                # there is not an abrupt cutoff with si, so feed it more data
                aid = np.nonzero(r < (8 * p))
                na = len(aid)
                a = np.zeros((na, 1), float)  # default to first guess (also called norm)
                # got to compute full thing
                from .supportingMethods import si_wt
                a = si_wt(y[aid], wt[aid])  # note, wt is %var that is signal
            else:
                aid = np.nonzero(r < p)[0]
                na = len(aid)
                a = np.zeros((na, 1), float)  # default to first guess (also called norm)
                f = interpolate.interp1d(ri, ai.squeeze(), 'linear')
                a = f(r[aid] / p)

                # apply a priori weights
                if (np.size(a) == 1):
                    a = a[0]
                elif (np.size(a) == 0):
                    a = 0
                if (np.size(aid) != 0):
                    a = a * wt[aid].squeeze()  # need to squeeze because wts are multidimensional

                suma = np.sum(a)
                if (abs(suma) > 0):
                    a = a / suma

            # check weights
            if (np.sum(a) > 0):
                # compute error, noise passed, fraction of target variance not explained
                #            (noise)  +  (    lost signal    )
                nmsei[i] = np.dot(a.conj().T, a) * (1 - np.spacing(1)) # want to distinguiush points with at least one nonzero weight
                # actual error is the noise passed (nmsei*s) + signal reduced (1-nmsei)*s
            else:
                # weights all zero
                nmsei[i] = 1

        # account for deviation from target scales with fraction of wavenumber band used
        q = p ** (-m)
        nmsei[i] = (1 - q * (1 - nmsei[i]))

        # convolve against data
        a_ConjugateTranspose = np.reshape(a.conj().T, (1, len(a.conj().T)))
        zi[i] = np.dot(a_ConjugateTranspose, z[aid])

        if (nmsei[i] < 1 and na > 0):
            # and weighted residuals are
            # tmp = np.reshape(a, (len(a), 1))
            y = (z[aid] - zi[i]) * np.expand_dims(a, axis=1)  # tmp removed ... fixed by expanding dims
            # del tmp
            msri[i] = np.dot(y.conj().T, y) / nmsei[i][0]  # weighted mean square residual
            # which is not accurate if small dof, add estimate that converges to observation error for (na-1) -> 0
            msri[i] = ((na - 1) * msri[i] + np.dot(a_ConjugateTranspose, s[aid])[0, 0]) / na
            # weighted mean square residual
            msei[i] = msri[i] * nmsei[i] / (1 - nmsei[i])  # predicted mean square error
        else:
            # set it to one
            nmsei[i] = 1

    return zi, msei, nmsei, msri


def scalecInterp(x, z, s, xi, lx, filtername, nmseitol):
    """This is a stand-alone general purpose interpolator.
     It remvoes a linear (or planar) trend first and then calls scalecInterpPerturbations 
    
     Args:
         x: the nxm location of the data- repeated indices not handled well (ignored)
         z: the observations
         s: the the observation errors (i.e., standard deviations, rms, etc.) s is used to weight the observations as 1/s
             choose s=0 if no information is available about the weights

         xi: the interpolation locations
         lx: the interpolation weighting length scales
         filtername: the name of a filter to analyze: ['quadloess', 'linloess',  'hanning', 'boxcar']

         nmseitol: a maximum error level, if exceeded causes doubling of smoothing scales
             NOTE: if nmseitol=1 then this means we accept result with input scales
                 if nmseitol<1 then this means interpolation will successive doubling of scales to reach desired noise reduction


     Returns
       zi: the estimate
       msei: the mean square interpolation error estimate (units of z)
       nmsei: the normalized mean square error
       msri: the mean square residuals

    """
    # fix up input data
    Ni, mi = np.shape(xi)
    N, m = np.shape(x)

    # deal with nans
    if (np.size(s) == 1):
        s = np.tile(s, (N, 1))

    tmp = np.concatenate((x, z, s), axis=1)
    idd = np.nonzero(np.isfinite(np.sum(tmp, axis=1)))
    x = x[idd, :]
    x = x[0]  # some dimensional wierdness happens with the above line of code...
    z = z[idd]
    s = s[idd]

    if (np.size(lx) == N and np.size(lx) != Ni):
        # got to remove the corresponding scales
        lx = lx[idd, :]
    N, m = np.shape(x)

    # need to remove trend once, # first, shift and scale grid and data
    mean_xi = np.mean(xi, axis=0)  # center on output center
    x = x - np.tile(mean_xi, (N, 1))
    xi = xi - np.tile(mean_xi, (Ni, 1))
    tmp = np.array([])
    for i in range(0, m):
        tmp = np.append(tmp, np.std(x[:, i]))
    std_x = tmp
    del tmp
    idd = np.nonzero(std_x == 0)
    if (np.size(idd) > 0):  # catch variables with zero variance (e.g., a profile)
        std_x[idd] = 1

    L = np.diag(1 / std_x)
    x = np.dot(x, L)
    xi = np.dot(xi, L)
    lx = np.dot(lx, L)

    # need some consistent weights
    wtol = 0.01
    from .supportingMethods import consistentWeight
    wt, var_z = consistentWeight(z, s ** 2, wtol)

    # do regression to remove a norm field
    btrend = np.zeros((mi + 1, 1), float)
    bi = btrend
    # only compute against variables with variance
    varid = np.nonzero(std_x > 0)
    if (np.size(varid) > 0):
        # this is just for getting the data ready, so it is meant to be bullet proof, not statistically pure!
        tmp1 = np.reshape(x[:, varid], (N, m))
        tmp2 = np.concatenate((np.ones((N, 1), float), tmp1), axis=1)
        from .supportingMethods import regr_xzw
        (btrend[0], btrend[varid[0][0] + 1], btrend[varid[0][1] + 1]), (
        bi[0], bi[varid[0][0] + 1], bi[varid[0][1] + 1]) = regr_xzw(tmp2, z, wt)
        print('removed order ', 1, ' polynomial \n')
        del tmp1, tmp2

    # regression failed if nan
    btrendHasNan = False
    for i in btrend:
        if np.isnan(i):
            btrendHasNan = True
    if (btrendHasNan == True):
        # pad with zero
        btrend = np.zeros((len(btrend), 1), float)

    tmp = np.concatenate((np.ones((N, 1), float), x), axis=1)
    ztrend = np.dot(tmp, btrend)
    del tmp

    # compute deviations from trend
    z = z - ztrend

    # pass the trend-removed data to scalecInterpPerturbations
    from .scalecInterpolation import scalecInterpPerturbations
    zi, msei, nmsei, msri = scalecInterpPerturbations(x, z, s, xi, lx, filtername, nmseitol, Ntotal=Ni, Ndone=0)

    # compute trend on data locations
    tmp = np.concatenate((np.ones((Ni, 1), float), xi), axis=1)
    ztrend = np.dot(tmp, btrend)
    del tmp

    # replace trend
    zi = zi + ztrend

    return zi, msei, nmsei, msri

def scalecInterpTilePerturbations(x, z, s, xi, lx, filtername, nmseitol):
    """optimize interpolation for regular grid output by breaking into bite-sized tiles
     which are passed to scalecInterpPerturbations (which does not remove any trend)
    
     Args:
         x: the nxm location of the observation data- repeated indices not handled well (ignored)
         z: the observations
         s; the the observation errors (i.e., standard deviations, rms, etc.)
             s is used to weight the observations as 1/s
             choose s=0 if no information is available about the weights

         xi: the interpolation locations
         lx: the interpolation weighting length scales
         filtername: the name of a filter to analyze:
            'quadloess'
            'linloess'
            'hanning'
            'boxcar'

         nmseitol: a maximum error level, if exceeded causes doubling of smoothing scales
           NOTE: if nmseitol=1 then this means we accept result with input scales
                 if nmseitol<1 then this means interpolation will successive doubling of scales to reach desired noise reduction

     Returns
         zi: the estimate
         msei: the mean square interpolation error estimate (units of z)
         nmsei: the normalized mean square error
         msri: the mean square residuals

     change log
     12 Feb 2009, NGP,  disabled the figure display so large regions don't croak

    Args:
      x: param z:
      s: param xi:
      lx: param filtername:
      nmseitol: 
      z: 
      xi: 
      filtername: 

    Returns:

    """
    if np.shape(lx) == np.shape(xi):
        lx_tog = 1
    else:
        lx_tog = 0

    # check dimensions
    Ni, mi = np.shape(xi)

    if (nmseitol == None):  # set None as default input
        # nmseitol = inf # never invoke tolerance
        nmseitol = 1 / (mi**2)  # default tol ensures some data used
        print('scalecInterpTile: setting maximum nmsei tolerance to ', nmseitol, '\n')

    # detect gridded data shape, indices of grid edges
    # SB - These were labeled backwards,causing errors
    idyi = np.squeeze(np.nonzero(xi[:, 0] == xi[0, 0]))  # the head of every x row
    idxi = np.squeeze(np.where(xi[:, 1] == xi[0, 1]))

    # if second occurrance of x(1)  does not coincide with end of occurrances of y(1), suspect Not gridded data
    if (idyi[1] - 1 != idxi[-1]):  # no evidence for grid, must interpolate all at once
        # send to stand-alone interp
        print('output not a 2-d grid\n')
        #from scalecInterpolation import scalecInterp
        ZI, MSEI, NMSEI, MSRI = scalecInterp(x, z, s, xi, lx, filtername, nmseitol)
        return ZI, MSEI, NMSEI, MSRI
    # calculate ni, nj (grid cell count)
    nyi =  idyi.shape[0]  # idxi[-1] + 1  - SBchanged (these were backwards, because of the way python flattens is different than that of matlab
    nxi =  idxi.shape[0]  # Ni / nyi
    # print 'nyi and nxi are backwards from super function, is this a problem i don''t know ' - doesn't seem to be SB
    # modify to handle time input for single time output on 2-d-h grid
    tmp = np.reshape(np.fix(nxi), (1))  # np.fix returns a 0-d array, so it must be reshaped to a 1-d array
    if (nxi != tmp[0]):
        print('indices not consistent with gridded output, continuing to interp \n')
        # send to stand-alone interp
        ZI, MSEI, NMSEI, MSRI = scalecInterp(x, z, s, xi, lx, filtername, nmseitol)
        return ZI, MSEI, NMSEI, MSRI
    else:
        # we still think we have gridded data
        Xi = np.reshape(xi[:,0], (nyi, nxi)) # changed shape from (nxi, nyi).T to (nyi, nxi)  - SB 8/7/17
        Yi = np.reshape(xi[:,1], (nyi, nxi)) # changed shape from (nxi, nyi).T to (nyi, nxi)
        #  and we are requesting single time

        if (mi == 3 and all(xi[:, 2] == xi[0, 2])):
            Ti = xi[0, 2]
        xitest = Xi[0, :]
        yitest = Yi[:, 0]
        Xitest, Yitest = np.meshgrid(xitest, yitest)
        # check carefully and pass to scalecInterp if fail
        if (not np.all(Xi == Xitest) and np.all(Yi == Yitest)):
            print('inidices not consistent with gridded output, continuing to interp \n')
            # send to stand-alone interp
            # from scalecInterpolation import scalecInterp
            ZI, MSEI, NMSEI, MSRI = scalecInterp(x, z, s, xi, lx, filtername, nmseitol)
            return ZI, MSEI, NMSEI, MSRI
    del tmp
    # if we survived to here, we have gridded output
    # xi and yi are now row,col vectors of grid indices
    xi = np.reshape(xitest, (1, len(xitest)))
    yi = np.reshape(yitest, (len(yitest), 1))
    del xitest, yitest, Xitest, Yitest

    # fix up input data
    N, m = np.shape(x)

    # deal with nans
    tmp1 = np.concatenate((x, z, s), axis=1)
    idd = np.nonzero(np.isfinite(np.sum(tmp1, axis=1)))
    x = x[idd[0], :]
    z = z[idd]
    s = s[idd]
    if (np.size(lx) == N and np.size(lx) != Ni):
        # got to remove the corresponding scales
        if (all(np.shape(lx) == np.shape(x))):  # May not need term 'all'
            print('smoothing scales vary with data')
            lx = lx[idd]
    N, m = np.shape(x)

    # need to remove trend once,
    # first, shift and scale grid and data
    # shift, center on output center
    mean_xi = np.mean(xi)
    mean_yi = np.mean(yi)
    xi = xi - mean_xi
    yi = yi - mean_yi
    Xi = Xi - mean_xi
    Yi = Yi - mean_yi
    x[:, 0] = x[:, 0] - np.repeat(mean_xi, N)
    x[:, 1] = x[:, 1] - np.repeat(mean_yi, N)

    # scale
    tmp = np.array([])
    for i in range(0, m):
        tmp = np.append(tmp, np.std(x[:, i]))
    std_x = tmp
    del tmp
    idd = np.nonzero(std_x == 0)
    if (np.size(idd) > 0):  # catch variables with zero variance (e.g., a profile)
        std_x[idd] = 1
    L = np.diag(1 / std_x)
    x = np.dot(x, L)
    xi = xi * L[0, 0]
    yi = yi * L[1, 1]
    Xi = Xi * L[0, 0]
    Yi = Yi * L[1, 1]
    # fix smoothing scales lx to scaled values
    lx = np.dot(lx, L)  # Matrix multiplication, not element-wise multiplication
    # add time
    if (mi == 3):
        Ti = Ti * L[2, 2]
        # need some consistent weights
    wtol = 0.01
    try:
        wt, var_z = consistentWeight(z, s ** 2, wtol)
    except:
        print('setting var_z = s^2')
        # if this dies, var_z = s.^2 is a nice guess
        var_z = s ** 2

    # sort out the lx input
    if (np.size(lx[:]) == mi):
        # same everywhere
        lxflag = 'constant'
    elif (np.size(lx[:, 1], axis=0) == Ni):
        lxflag = 'ongrid'
        lxgrid = np.reshape(lx[:, 0], (nyi, nxi))
        lygrid = np.reshape(lx[:, 1], (nyi, nxi))
        if (mi == 3):
            ltgrid = np.reshape(lx[:, 2], (nyi, nxi));
    else:
        # assume on data
        lxflag = 'ondata'

    # get optimal tile
    # first compute smoothness/domain size ratio
    if lx_tog:
        lk = np.max(
            np.array([np.min(lx[:, 0]), np.min(lx[:, 1])]) / np.array([np.max(xi) - np.min(xi), np.max(yi) - np.min(yi)]))
    else:
        lk = np.max(np.array([np.min(lx[0]), np.min(lx[1])]) / np.array([np.max(xi) - np.min(xi), np.max(yi) - np.min(yi)]))

    if (np.isnan(lk) or np.isinf(lk)):
        lk = 0
    # next, the optimal number of tiles
    kopt = np.sqrt(nxi * nyi * (1+lk))
    # number of tiles in x,y dims
    kx = np.ceil(np.sqrt(kopt))  # proportion to grid dimensions
    ky = np.ceil(kopt / kx)
    kopt = kx * ky
    ropt = (1 / kopt) + kopt / np.sqrt((1 + lk) * nxi * nyi)

    # divide grid points per tile, roughly
    if (kx == 1):
        nkx = nxi
    else:
        nkx = float(int(nxi / kx)) # must round down # NOTE: float(int()) is a fix for numpy.fix returning a 0-d array that can't be indexed... matlab fix doesn't do that...
        if(nkx < 1):
            nkx = 1
            kx = nxi  # make sure we do them all

    if (ky == 1):
        nky = nyi
    else:
        nky = float(int(nyi / ky))
        if (nky < 1):
            nky = 1
            ky = nyi
    # print 'number of tiles = ', kopt ,', expected efficiency = %.4f' %ropt ,', xi/tile = ', nkx ,', yi/tile = ', nky
    # specify overlap
    if lx_tog:
        Lmax = 10 * np.array([max(lx[:, 0]), max(lx[:, 1])])
    else:
        Lmax = 10 * np.array([lx[0], lx[1]])

    # init output
    ZI = np.ma.array(np.ones((nyi,nxi), dtype=float), mask=True)    # init output differently than original
    NMSEI = np.ma.array(np.ones((nyi,nxi), dtype=float), mask=True) # np.ones((nyi, nxi), float)
    #  this one is NOT MASKED in Meg's code!!!!!

    MSEI = np.ma.array(np.ones((nyi,nxi), dtype=float), mask=True)  # np.nan * np.ones((nyi,nxi), float)
    MSRI = np.ma.array(np.ones((nyi,nxi), dtype=float), mask=True)  # np.nan * np.ones((nyi,nxi), float)
    # begin Interp
    Ndone = 0
    for i in range(0, int(kx)):
        idxi = np.arange(0, int(nkx)) + i * int(nkx)  # indices to interp this time
        if (i == kx - 1 and idxi[-1] != nxi):
            idxi = np.arange(idxi[0], nxi)  # catch the end here
        # what is appropriate overlap?
        xmin = xi[0, idxi[0]] - Lmax[0]  # find tile limits
        xmax = xi[0, idxi[-1]] + Lmax[0]
        idx = np.where((x[:, 0] > xmin) & (x[:, 0] < xmax))[0]  # get the useful data
        if (len(idxi) > 0):
            # repeat at each yi
            for j in range(0, int(ky)):  # Loop through each Y
                idyi = np.arange(0, int(nky)) + j * int(nky) # indices to interp this time
                if(j == ky-1 and idyi[-1] != nyi):
                    idyi = np.arange(idyi[0], nyi) # catch the end here
                ymin = yi[idyi[0],0] - Lmax[1]
                ymax = yi[idyi[-1],0] + Lmax[1]
                idxy = idx[np.where((x[idx,1] < ymax) & (x[idx,1] > ymin))[0]]
                if(np.size(idxy) > mi):

                    # send to interpolator
                    tmp1, tmp2 = np.meshgrid(idyi, idxi)
                    Xii = Xi[tmp1,tmp2].T
                    Yii = Yi[tmp1,tmp2].T

                    # deal with smoothing scales
                    if (lxflag == 'constant'):
                        L = lx
                    elif (lxflag == 'ongrid'):
                        lxii = lxgrid[tmp1, tmp2].T
                        lyii = lygrid[tmp1, tmp2].T
                        L = np.array([lxii.flatten(), lyii.flatten(1)]).T
                        if (mi == 3):
                            ltii = ltgrid[tmp1, tmp2].T
                            ltii = np.reshape(ltii, (np.size(ltii), 1))
                            L = np.concatenate((L, ltii), axis=1)
                    elif (lxflag == 'ondata'):
                        L = lx[idxy]
                    # now, send to interp
                    if (mi == 2):
                        # interp 2-d
                        # from scalecInterpolation import scalecInterpPerturbations
                        zi, msei, nmsei, msri = scalecInterpPerturbations(x[idxy, :], z[idxy], s[idxy], np.array([Xii.flatten(1), Yii.flatten(1)]).T, L, filtername, nmseitol, Ni, Ndone)
                    elif (mi == 3):
                        # interp 2-d + time
                        # from scalecInterpolation import scalecInterpPerturbations
                        zi, msei, nmsei, msri = scalecInterpPerturbations(x[idxy,:], z[idxy], s[idxy], np.array([Xii.flatten(1), Yii.flatten(1), Ti+0*Xii.flatten(1)]).T, L, filtername, nmseitol, Ni, Ndone)
                    zi = np.reshape(zi, (len(idxi), len(idyi))).T
                    msei = np.reshape(msei, (len(idxi), len(idyi))).T
                    nmsei = np.reshape(nmsei, (len(idxi), len(idyi))).T
                    msri = np.reshape(msri, (len(idxi), len(idyi))).T

                    # this has been changed from the below?  I don't know how the variable names were mixed up
                    ZI[idyi[0]:idyi[-1] + 1, idxi[0]:idxi[-1] + 1] = zi
                    MSEI[idyi[0]:idyi[-1] + 1, idxi[0]:idxi[-1] + 1] = msei
                    NMSEI[idyi[0]:idyi[-1] + 1, idxi[0]:idxi[-1] + 1] = nmsei
                    MSRI[idyi[0]:idyi[-1] + 1, idxi[0]:idxi[-1] + 1] = msri

                    Ndone = Ndone + len(idyi) * len(idxi)
                    del tmp1, tmp2

    # return output in cols
    ZI    = ZI.flatten(1)
    MSEI  = MSEI.flatten(1)
    NMSEI = NMSEI.flatten(1)
    MSRI  = MSRI.flatten(1)

    # fix up error too - SB redone-- asked by original comments
    idd = np.argwhere(MSEI.mask == True).squeeze()
    if (np.size(idd) > 0):
        MSEI[idd] = var_z + np.mean(s**2)
        MSRI[idd] = MSEI[idd]

    return ZI, MSEI, NMSEI, MSRI

def scalecInterpTile(x, z, s, xi, lx, filtername, nmseitol):
    """ remove a plane-trend and pass to scalecInterpTilePerturbations (which does not remove any trend)
     
     Args
       x: the nxm location of the data- repeated indices not handled well (ignored)
       z: the observations
       s: the the observation errors (i.e., standard deviations, rms, etc.) s is used to weight the observations as 1/s
           choose s=0 if no information is available about the weights

       xi: the interpolation locations
       lx: the interpolation weighting length scales
       filtername: the name of a filter to analyze:
           'quadloess'
           'linloess'
           'hanning'
           'boxcar'

       nmseitol: a maximum error level, if exceeded causes doubling of smoothing scales
           NOTE: if nmseitol=1 then th    is means we accept result with input scales
                 if nmseitol<1 then this means interpolation will successive doubling of scales to reach desired noise reduction

     
     Returns:
       zi: the estimate
       msei: the mean square interpolation error estimate (units of z)
       nmsei: the normalized mean square error
       msri: the mean square residuals
    
     modifications by updating the trend only where valid perturbations were interpolated

    """
    # catch  input
    Ni, mi = np.shape(xi)
    N, m = np.shape(x)

    # need some consistent weights
    wtol = 0.01
    from .supportingMethods import consistentWeight
    wt, var_z = consistentWeight(z, s ** 2, wtol)

    # do regression to remove a norm field
    btrend = np.zeros((mi + 1, 1), float)
    bi = btrend

    # only compute against variables with variance
    tmp = np.array([])
    for i in range(0, m):
        tmp = np.append(tmp, np.std(x[:, i]))
    std_x = tmp
    del tmp
    idd = np.nonzero(std_x == 0)
    if (np.size(idd) > 0): # catch variables with zero variance (e.g., a profile)
        std_x[idd] = 1

    varid = np.nonzero(std_x > 0)
    if (np.size(varid) > 0):
        # this is just for getting the data ready, so it is meant to be bullet proof, not statistically pure!
        tmp1 = np.reshape(x[:,varid], (N,m))
        tmp2 = np.concatenate((np.ones((N,1), float), tmp1), axis=1)
        from .supportingMethods import regr_xzw
        (btrend[0], btrend[varid[0][0]+1], btrend[varid[0][1]+1]), (bi[0], bi[varid[0][0]+1], bi[varid[0][1]+1]) = regr_xzw(tmp2, z, wt)
        print('removed order ', 1 ,' polynomial \n')
        del tmp1, tmp2

    # regression failed if nan
    for i in btrend:
        if (np.isnan(i)):
            btrendHasNan = True
    if (btrendHasNan):
        # pad with zero
        btrend = np.zeros((len(btrend), 1), float)

    # remove trend
    tmp = np.concatenate((np.ones((N,1), float), x), axis=1)
    if (np.size(btrend, axis=0) != np.size(tmp,axis=1)): # check to see if the axes of btrend and tmp are compatible for matrix multiplication
        nbt, mbt = np.shape(btrend) # if they aren't, identify the axes of btrend
        diffAxis = np.size(tmp,axis=0) - np.size(btrend, axis=0) # obtain the difference between the two incompatible axes
        diffArray = np.zeros((diffAxis, mbt)) # make an array to be used as padding for btrend that is of length of the differnce of the incompatible axes
        btrend = np.append(btrend, diffArray)#, axis=1) # append the padding array
        btrend = np.reshape(btrend, (np.size(btrend, axis=0),1))
        #btrend = np.reshape(btrend, (1, np.size(btrend,axis=0))) # flip the indices of btrend for matrix multiplication 
    ztrend = np.dot(tmp, btrend) # finally... multiply the matrices
    z = z - ztrend 

    # send to interp tile perturbations
    from .scalecInterpolation import scalecInterpTilePerturbations
    Zi, Msei, Nmsei, Msri = scalecInterpTilePerturbations(x, z, s, xi, lx, filtername='hanning', nmseitol=nmseitol)#, WB)

    # replace the trend - except where useless
    tmp = np.concatenate((np.ones((Ni, 1), float), xi), axis=1)
    ztrend = np.dot(tmp, btrend)
    ztrend = np.reshape(ztrend, np.size(Zi))
    idgood = np.where(Nmsei != 1) and np.where(Nmsei != 0)
    Zi = Zi.flatten(1)
    Zi[idgood] = Zi[idgood] + ztrend[idgood]

    return Zi, Msei, Nmsei, Msri
